{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Markov Model\n",
    "Based on \"Structured Inference Networks for Nonlinear State Space Models\" by Krishnan, Shalit and Sontag. (AAAI 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax.experimental import stax\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import SVI, ELBO\n",
    "from numpyro.infer.stein import SVGD\n",
    "from numpyro.infer.kernels import RBFKernel\n",
    "from numpyro.guides import WrappedGuide\n",
    "from numpyro.optim import Adam\n",
    "from numpyro.examples.datasets import load_dataset, JSBCHORALES\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "init, get_batch = load_dataset(JSBCHORALES, batch_size=32)\n",
    "ds_count, ds_indxs = init()\n",
    "seqs, seqs_rev, lengths = get_batch(0, ds_indxs)\n",
    "print(\"Sequences: \", seqs.shape)\n",
    "print(\"Length min: \", min(lengths), \"max: \", max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _one_hot_chorales(seqs, num_nodes=88):\n",
    "    return np.sum(np.array((seqs[..., None] == np.arange(num_nodes + 1)), 'int'),axis=-2)[..., 1:]\n",
    "_one_hot_chorales(seqs[:, 0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reverse_padded(padded, lengths):\n",
    "    def _reverse_single(p, l):\n",
    "        new = np.zeros_like(p)\n",
    "        reverse = np.roll(p[::-1], l, axis=0)\n",
    "        return jax.ops.index_update(new, jax.ops.index[:], reverse)\n",
    "    return jax.vmap(_reverse_single)(padded, lengths)\n",
    "with jax.disable_jit():\n",
    "    print(_reverse_padded(np.array([[[1, 2], [2, 3], [3, 4], [4, 5]] + [[0, 0]], [[8, 9],[9,10],[10,11]] + [[0,0]] * 2]), np.array([4, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMM Neural Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emitter(hidden_dim1, hidden_dim2, out_dim):\n",
    "    return stax.serial(\n",
    "        stax.Dense(hidden_dim1), stax.Relu,\n",
    "        stax.Dense(hidden_dim2), stax.Relu,\n",
    "        stax.Dense(out_dim), stax.Sigmoid\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transition(gate_hidden_dim, prop_mean_hidden_dim, out_dim):\n",
    "    gate_init_fun, gate_apply_fun = stax.serial(\n",
    "        stax.Dense(gate_hidden_dim), stax.Relu,\n",
    "        stax.Dense(out_dim), stax.Sigmoid\n",
    "    )\n",
    "\n",
    "    prop_mean_init_fun, prop_mean_apply_fun = stax.serial(\n",
    "        stax.Dense(prop_mean_hidden_dim), stax.Relu,\n",
    "        stax.Dense(out_dim)\n",
    "    )\n",
    "\n",
    "    mean_init_fun, mean_apply_fun = stax.Dense(out_dim)\n",
    "\n",
    "    stddev_init_fun, stddev_apply_fun = stax.serial(\n",
    "        stax.Relu, stax.Dense(out_dim),\n",
    "        stax.Softplus\n",
    "    )\n",
    "\n",
    "    def init_fun(rng, input_shape):\n",
    "        output_shape = input_shape[:-1] + (out_dim,)\n",
    "        k1, k2, k3, k4 = jax.random.split(rng, num=4)\n",
    "        _, gate_params = gate_init_fun(k1, input_shape)\n",
    "        prop_mean_output_shape, prop_mean_params = prop_mean_init_fun(k2, input_shape)\n",
    "        _, mean_params = mean_init_fun(k3, input_shape)\n",
    "        _, stddev_params = stddev_init_fun(k4, prop_mean_output_shape)\n",
    "        return (output_shape, output_shape), (gate_params, prop_mean_params, \n",
    "                                              mean_params, stddev_params)\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        gate_params, prop_mean_params, mean_params, stddev_params = params\n",
    "        gt = gate_apply_fun(gate_params, inputs)\n",
    "        ht = prop_mean_apply_fun(prop_mean_params, inputs)\n",
    "        mut = (1 - gt) * mean_apply_fun(mean_params, inputs) + gt * ht\n",
    "        sigmat = stddev_apply_fun(stddev_params, ht)\n",
    "        return mut, sigmat\n",
    "    \n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Combiner(hidden_dim, out_dim):\n",
    "    mean_init_fun, mean_apply_fun = stax.Dense(out_dim)\n",
    "\n",
    "    stddev_init_fun, stddev_apply_fun = stax.serial(\n",
    "        stax.Dense(out_dim),\n",
    "        stax.Softplus\n",
    "    )\n",
    "\n",
    "    def init_fun(rng, input_shape):\n",
    "        output_shape = input_shape[:-1] + (out_dim,)\n",
    "        k1, k2 = jax.random.split(rng, num=2)\n",
    "        _, mean_params = mean_init_fun(k1, input_shape)\n",
    "        _, stddev_params = stddev_init_fun(k2, input_shape)\n",
    "        return (output_shape, output_shape) , (mean_params, stddev_params)\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        mean_params, stddev_params = params\n",
    "        mut = mean_apply_fun(mean_params, inputs)\n",
    "        sigmat = stddev_apply_fun(stddev_params, inputs)\n",
    "        return mut, sigmat\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU(hidden_dim, W_init=stax.glorot_normal()):\n",
    "    # Inspired by https://github.com/google/jax/pull/2298\n",
    "    input_update_init_fun, input_update_apply_fun = stax.Dense(hidden_dim)\n",
    "    input_reset_init_fun, input_reset_apply_fun = stax.Dense(hidden_dim)\n",
    "    input_output_init_fun, input_output_apply_fun = stax.Dense(hidden_dim)\n",
    "\n",
    "    def init_fun(rng, input_shape):\n",
    "        indv_input_shape = input_shape[1:]\n",
    "        output_shape = input_shape[:-1] + (hidden_dim,)\n",
    "        rng, k1, k2 = jax.random.split(rng, num=3)\n",
    "        hidden_update_w = W_init(k1, (hidden_dim, hidden_dim))\n",
    "        _, input_update_params = input_update_init_fun(k2, indv_input_shape)\n",
    "\n",
    "        rng, k1, k2 = jax.random.split(rng, num=3)\n",
    "        hidden_reset_w = W_init(k1, (hidden_dim, hidden_dim))\n",
    "        _, input_reset_params = input_reset_init_fun(k2, indv_input_shape)\n",
    "\n",
    "        rng, k1, k2 = jax.random.split(rng, num=3)\n",
    "        hidden_output_w = W_init(k1, (hidden_dim, hidden_dim))\n",
    "        _, input_output_params = input_output_init_fun(k2, indv_input_shape)\n",
    "\n",
    "        return output_shape, (hidden_update_w, input_update_params,\n",
    "                              hidden_reset_w, input_reset_params,\n",
    "                              hidden_output_w, input_output_params)\n",
    "    \n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        print(np.shape(params))\n",
    "        (hidden_update_w, input_update_params,\n",
    "         hidden_reset_w, input_reset_params,\n",
    "         hidden_output_w, input_output_params) = params\n",
    "        inps, lengths, init_hidden = inputs\n",
    "\n",
    "        def apply_fun_single(prev_hidden, inp):\n",
    "            i, inpv = inp\n",
    "            inp_update = input_update_apply_fun(input_update_params, inpv)\n",
    "            hidden_update = np.dot(prev_hidden, hidden_update_w)\n",
    "            update_gate = stax.sigmoid(inp_update + hidden_update)\n",
    "            reset_gate = stax.sigmoid(input_reset_apply_fun(input_reset_params, inpv) +\n",
    "                                      np.dot(prev_hidden, hidden_reset_w))\n",
    "            output_gate = update_gate * prev_hidden + (1 - update_gate) * np.tanh(\n",
    "                input_output_apply_fun(input_output_params, inpv) + \n",
    "                np.dot(reset_gate * prev_hidden, hidden_output_w))\n",
    "            hidden = np.where((i < lengths)[:, None], output_gate, np.zeros_like(prev_hidden))\n",
    "            return hidden, hidden\n",
    "\n",
    "        return jax.lax.scan(apply_fun_single, init_hidden, (np.arange(inps.shape[0]), inps))\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic model and guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a vectorized version based on https://github.com/pyro-ppl/pyro/blob/f73df6c1c20bc7b9164d79ce4217557d0aa8e396/examples/dmm/dmm.py#L192 by Martin Jankowiak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(seqs, seqs_rev, lengths, *,\n",
    "          latent_dim=100, emission_dim=100, transition_dim=200,\n",
    "          data_dim=88, gru_dim=400, annealing_factor=1.0):\n",
    "    batch_size, max_seq_length, *_ = seqs.shape\n",
    "    \n",
    "    transition = numpyro.module('transition', Transition(transition_dim, transition_dim, latent_dim),\n",
    "                                input_shape=(batch_size, latent_dim))\n",
    "    emitter = numpyro.module('emitter', Emitter(emission_dim, emission_dim, data_dim),\n",
    "                                input_shape=(batch_size, latent_dim))\n",
    "\n",
    "    z0 = numpyro.param('z0', np.zeros((batch_size, 1, latent_dim)))\n",
    "    ones = np.ones((batch_size, max_seq_length, latent_dim))\n",
    "\n",
    "    masks = np.repeat(np.expand_dims(np.arange(max_seq_length), axis=0), batch_size, axis=0) <\\\n",
    "            np.expand_dims(lengths, axis=-1)\n",
    "    with numpyro.plate('data', batch_size):\n",
    "        # NB: Mask is to avoid scoring 'z' using distribution at this point\n",
    "        z = numpyro.sample('z', dist.Normal(0.0, ones).mask(False).to_event(2))\n",
    "        z_shift = np.concatenate([z0, z[:, :-1, :]], axis=-2)\n",
    "        z_loc, z_scale = transition(z_shift)\n",
    "\n",
    "        with numpyro.handlers.scale(scale_factor=annealing_factor):\n",
    "            # Actually score 'z'\n",
    "            numpyro.sample('z_aux', dist.Normal(z_loc, z_scale).mask(np.expand_dims(masks, axis=-1))\n",
    "                                                               .to_event(2), obs=z)\n",
    "        \n",
    "        emission_probs = emitter(z)\n",
    "        oh_x = _one_hot_chorales(seqs)\n",
    "        numpyro.sample('obs_x', dist.Bernoulli(emission_probs).mask(np.expand_dims(masks, axis=-1))\n",
    "                                                              .to_event(2), obs=oh_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(seqs, seqs_rev, lengths, *,\n",
    "          latent_dim=100, emission_dim=100, transition_dim=200,\n",
    "          data_dim=88, gru_dim=400, annealing_factor=1.0):\n",
    "    batch_size, max_seq_length, *_ = seqs.shape\n",
    "    seqs_rev = np.transpose(seqs_rev, axes=(1, 0, 2))\n",
    "    print(seqs_rev.shape)\n",
    "    gru = numpyro.module('gru', GRU(gru_dim), input_shape=(max_seq_length, batch_size, data_dim))\n",
    "    combiner = numpyro.module('combiner', Combiner(gru_dim, latent_dim),\n",
    "                              input_shape=(batch_size, gru_dim))\n",
    "    \n",
    "    masks = np.repeat(np.expand_dims(np.arange(max_seq_length), axis=0), batch_size, axis=0) <\\\n",
    "        np.expand_dims(lengths, axis=-1)\n",
    "\n",
    "    h0 = numpyro.param('h0', np.zeros((batch_size, gru_dim)))\n",
    "    _, hs = gru((_one_hot_chorales(seqs_rev), lengths, h0))\n",
    "    hs = _reverse_padded(np.transpose(hs, axes=(1, 0, 2)), lengths)\n",
    "    z_loc, z_scale = combiner(hs)\n",
    "    with numpyro.plate('data', batch_size):\n",
    "        with numpyro.handlers.scale(scale_factor=annealing_factor):\n",
    "            numpyro.sample('z', dist.Normal(z_loc, z_scale).mask(np.expand_dims(masks, axis=-1)).to_event(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(model, guide, Adam(8e-4), ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "rng_key = jax.random.PRNGKey(seed=142)\n",
    "state = svi.init(rng_key, seqs, seqs_rev, lengths)\n",
    "pbar = tqdm(range(num_epochs * ds_count))\n",
    "for j in pbar:\n",
    "    i = j % num_epochs\n",
    "    seqs, seqs_rev, lengths = get_batch(i, ds_indxs)\n",
    "    state, loss = jax.jit(svi.update)(state, seqs, seqs_rev, lengths)\n",
    "    pbar.set_description(f'SVI loss: {loss}', True)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stein Variational Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svgd = SVGD(model, WrappedGuide(guide), Adam(8e-4), ELBO(), RBFKernel(), num_stein_particles=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "rng_key = jax.random.PRNGKey(seed=142)\n",
    "state = svgd.init(rng_key, seqs, seqs_rev, lengths)\n",
    "pbar = tqdm(range(num_epochs * ds_count))\n",
    "for j in pbar:\n",
    "    i = j % num_epochs\n",
    "    seqs, seqs_rev, lengths = get_batch(i, ds_indxs)\n",
    "    state, loss = jax.jit(svgd.update)(state, seqs, seqs_rev, lengths)\n",
    "    pbar.set_description(f'SVGD loss: {loss}', True)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitae5f2af3297f41dd9420774d1e1722e6",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
